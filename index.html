<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <title>Jinghao Zhou's homepage</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!-- Le styles -->
    <link href="./css/bootstrap.min.css" rel="stylesheet">
    <link href="./css/bootstrap-responsive.min.css" rel="stylesheet">
    <link href="./css/main.css" rel="stylesheet">

    <!-- Le HTML5 shim, for IE6-8 support of HTML5 elements -->
    <link rel="icon" href="./assets/ico/favicon.ico">
</head>


<body>


  <div class="container">

    <div id="floating_sidebar" class="span3 hidden-phone">
      <!-- We use a fancy nav bar if there is enough space -->
      <!--<hr class="hidden-phone">-->
      <br>
      <ul class="nav nav-list bs-docs-sidenav hidden-phone">
        <li><a href="#top">Bio </a></li>
        <li><a href="#publications">Publications  </a></li>
        <li><a href="#projects">Projects  </a></li>
        <li><a href="#researchinterests">Research Interests  </a></li>
      </ul>

      <hr class="hidden-phone">
      <div class="text-center hidden-phone">
        <img src="assets/ico/me.png" width=160px alt="photo" class="logo-image"><br><br>
        <p> Email: jensen dot zhoujh at gmail dot com</p>
      </div>
      <div id="footer">
        <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=080808&w=250&t=tt&d=SuyMPMY6OOgAhEdXDS6Xlc-HXmnw9Yb5sLKpDuhtW4s&co=ffffff&cmo=3acc3a&cmn=ff5353&ct=808080"></script>
      </div>
    </div>
    
    <div class="span9" id="main_body">
      <div class="visible-phone text-center" style="font-size:20px;">
        <br>
        <a href="#top">Bio</a> |
        <a href="#publications">Publications</a> |
        <a href="#projects">Projects</a> |
        <a href="#researchinterests">Research Interests</a>
      </div>
      <div id="toptitle">
	    <h1>Jinghao (Jensen) Zhou
          <div class="pull-right">
            <a href="https://twitter.com/jensenzhoujh"><img src="./assets/ico/twitter.png" width="28px"></a>
            <a href="https://github.com/shallowtoil"><img src="./assets/ico/GitHub-Mark.png" width="32px"></a>
            <a href="https://www.linkedin.com/in/jensenzhoujh/"><img src="./assets/ico/linkedin.png" width="22px"></a>
            <a href="https://www.zhihu.com/people/jensen.zhoujh/"><img src="./assets/ico/cssi_zhihu-512.png" width="22px"></a>
            <!-- <a href="https://instagram.com/jensenzhoujh"><img src="./assets/ico/ins.png" width="30px"></a> -->
            <!-- <a href="https://www.youtube.com/channel/UC9qa5a404zaVi2yK3qENlfQ"><img src="./assets/ico/youtube.png" width="28px"></a> -->
          </div>
        </h1>
      </div>

      <div class="visible-phone text-center">
        <center><img class="media-object" src="assets/ico/me.png" alt="alt-text" width="154px" style="margin: 0px 10px"></center>
        <p> Email: jensen dot zhoujh at gmail dot com</p>
        <br>
      </div>
      <div style="text-indent: 1.2em;"><p>
          I am currently an undergraduate student at <a target="_blank" href="https://www.nwpu.edu.cn/">Northwestern Polytechnical University</a>. 
          I am also a prospective research-oriented master student at <a target="_blank" href="https://www.ri.cmu.edu/">Robotic Institute, Carnegie Mellon University</a>.
          I had fun researching computer vision as interns at 
          <a target="_blank" href="https://ccvl.jhu.edu/">CCVL Lab</a>, 
          <a target="_blank" href="https://ailab.bytedance.com/">ByteDance AI Lab</a>, 
          and <a target="_blank" href="https://www.sensetime.com/en">SenseTime</a>.
      </p></div>

      <div class="infoblock">
        <p> NEWS: </p>
        <ul><p class="media-heading" >
	    <li>2021/4/11: Joining CCVL Lab as a research intern.</li>
            <li>2021/3/31: Joining ByteDance AI Lab as a research intern.</li>
            <li>2020/4/05: Joining SenseTime as a research intern.</li>
            <li>2019/11/11: One paper accepted at AAAI 2020.</li>
        </p></ul>
      </div>

      <!--
          *** Publications ***
        -->
      <h3>
        <a name="publications"></a> Selected Publications
      </h3>

      Full Publications: <a target="_blank" href="https://scholar.google.com/citations?user=AoDJADEAAAAJ&hl=en">Google Scholar</a><br> 
	  
      <!--<h4>Conference and Journals</h4> -->
      <div class="media">
        <div class="media-body">
          <p>2021</p>
          <ul><li><p class="media-heading">
                <i>
                  Higher Performance Visual Tracking with Dual-Modal Localization
                </i>
                [<a target="_blank"
                    href="https://arxiv.org/abs/2103.10089">pdf</a>] <br>
                <!-- [<a target="_blank"
                     href="https://github.com/shallowtoil/DROL">code</a>] <br> -->
                <strong>Jinghao Zhou</strong>, Bo Li, Lei Qiao, Peng Wang, Weihao Gan, Wei Wu, Junjie Yan, and Wanli Ouyang <br> 
                <i>Tech report</i>, arXiv, 2021
          </p></li></ul>
          <!--                     <p class="abstract-text">
                                   The problem of visual object tracking has traditionally been handled by variant tracking paradigms, either learning a model of the object's appearance exclusively online or matching the object with the target in an offline-trained embedding space. Despite the recent success, each method agonizes over its intrinsic constraint. The online-only approaches suffer from a lack of generalization of the model they learn thus are inferior in target regression, while the offline-only approaches (e.g., convontional siamese trackers) lack the video-specific context information thus are not discriminative enough to handle distractors. Therefore, we propose a parallel framework to integrate offline-trained siamese networks with a lightweight online module for enhance the discriminative capability. We further apply a simple yet robust template update strategy for siamese networks, in order to handle object deformation. Robustness can be validated in the consistent improvement over three siamese baselines: SiamFC, SiamRPN++, and SiamMask. Beyond that, our model based on SiamRPN++ obtains the best results over six popular tracking benchmarks. Though equipped with an online module when tracking proceeds, our approach inherits the high efficiency from siamese baseline and can operate beyond real-time.
                                   </p> -->

          <ul><li><p class="media-heading">
                <i>
                  Real-Time Visual Object Tracking via Few-Shot Learning
                </i>
                [<a target="_blank"
                    href="https://arxiv.org/abs/2103.10130">pdf</a>] <br>
                <!-- [<a target="_blank"
                     href="https://github.com/shallowtoil/DROL">code</a>] <br> -->
                <strong>Jinghao Zhou</strong>, Bo Li, Peng Wang, Peixia Li, Weihao Gan, Wei Wu, Junjie Yan, and Wanli Ouyang <br> 
                <i>Tech report</i>, arXiv, 2021
          </p></li></ul>
          <!--                     <p class="abstract-text">
                                   The problem of visual object tracking has traditionally been handled by variant tracking paradigms, either learning a model of the object's appearance exclusively online or matching the object with the target in an offline-trained embedding space. Despite the recent success, each method agonizes over its intrinsic constraint. The online-only approaches suffer from a lack of generalization of the model they learn thus are inferior in target regression, while the offline-only approaches (e.g., convontional siamese trackers) lack the video-specific context information thus are not discriminative enough to handle distractors. Therefore, we propose a parallel framework to integrate offline-trained siamese networks with a lightweight online module for enhance the discriminative capability. We further apply a simple yet robust template update strategy for siamese networks, in order to handle object deformation. Robustness can be validated in the consistent improvement over three siamese baselines: SiamFC, SiamRPN++, and SiamMask. Beyond that, our model based on SiamRPN++ obtains the best results over six popular tracking benchmarks. Though equipped with an online module when tracking proceeds, our approach inherits the high efficiency from siamese baseline and can operate beyond real-time.
                                   </p> -->
          
        </div>
      </div>


      <div class="media">
        <div class="media-body">
          <p>2020</p>
          <ul><li><p class="media-heading">
                <i>
                  Discriminative and Robust Online Learning for Siamese Visual Tracking
                </i>
                [<a target="_blank"
                    href="https://arxiv.org/abs/1909.02959">pdf</a>]  
                [<a target="_blank"
                    href="https://github.com/shallowtoil/DROL">code</a>] <br>
                <strong>Jinghao Zhou</strong>, Peng Wang, and Haoyang Sun. <br> 
                <i>AAAI Conference on Artificial Intelligence (AAAI)</i>, 2020
          </p></li></ul>
          <!--                     <p class="abstract-text">
                                   The problem of visual object tracking has traditionally been handled by variant tracking paradigms, either learning a model of the object's appearance exclusively online or matching the object with the target in an offline-trained embedding space. Despite the recent success, each method agonizes over its intrinsic constraint. The online-only approaches suffer from a lack of generalization of the model they learn thus are inferior in target regression, while the offline-only approaches (e.g., convontional siamese trackers) lack the video-specific context information thus are not discriminative enough to handle distractors. Therefore, we propose a parallel framework to integrate offline-trained siamese networks with a lightweight online module for enhance the discriminative capability. We further apply a simple yet robust template update strategy for siamese networks, in order to handle object deformation. Robustness can be validated in the consistent improvement over three siamese baselines: SiamFC, SiamRPN++, and SiamMask. Beyond that, our model based on SiamRPN++ obtains the best results over six popular tracking benchmarks. Though equipped with an online module when tracking proceeds, our approach inherits the high efficiency from siamese baseline and can operate beyond real-time.
                                   </p> -->
        </div>
      </div>



      <h3>
        <a name="projects"></a> Projects
      </h3> 
      
      <div class="media">
        <!-- <div class="pull-left">
             <img class="media-object" src="./assets/projects/IFMV.png" width="125px" height="125px">
        </div> -->
        <div class="media-body">
          <ul>
			<li><p class="media-heading">
                <i>
                  Information Fusion and Multi-dimention Vision on Wheeled Soccer Robot
                </i>
                [<a target="_blank"
                    href="https://drive.google.com/open?id=1S0gSsbb45Fvkxh00GnFxqjcCFB7RuSZf">slide</a>]
                [<a target="_blank"
                    href="https://github.com/hawking-npu/nubot_ws">code</a>]
              </p>
              <!-- <p class="abstract-text">
                   I designed a robost framework for our Middle Size League soccer-playing robots, which is majorly composed of omni-vision using GigE paranoma camera and front-vision using ZED RGB-D camera. This work achieved the real-time localization, mapping and object detection and it's done when I'm in <a target="_blank" href="https://github.com/hawking-npu">Wheeled Soccer Robot Base of Northwestern Polytechnical University@Hawking</a> for RoboCup competition.
                   </p> -->
          </li></ul>
        </div>
      </div>
      
      <!--
          *** Reseach Interests ***
        -->
      <h3>
        <a name="researchinterests"></a> Research Interests
      </h3>

      <div class="media">
        <h4>Meta Learning</h4>
        <!-- <div class="pull-left">
             <img class="media-object" src="./assets/cv/fewshot.jpg" width="150px" height="150px">
   		</div> -->
        <div class="media-body">
          <p class="abstract-text">
            Despite the recent success of deep neural networks, the high performance of these models often largely attributes to the large-volume training data, while the challenges still remain in the context of limited data. To address this issue, meta learning aims to learn a set of meta knowledge from data. In the area of supervised learning, this direction is also known as few-shot learning. 
          </p>
        </div>
      </div>

      <div class="media">
        <h4>Image-Level Recognition</h4>
        <!-- <div class="pull-left">
             <img class="media-object" src="./assets/cv/cls.png" width="150px" height="150px">
        </div> -->
        <div class="media-body">
          <p class="abstract-text">
            Image-level recognition preceive one image as a whole. Traditional image-level recognition resolves around <strong>image classification</strong>, or <strong>action recognition</strong> in a video level.
            Related topics in low data regime include <strong>image retrival (re-indentification)</strong>, and <strong>few-shot learning</strong>, etc.
            Other topics with limited labelled data, such as <strong>unsupervised learning</strong> and <strong>semisupervised learning</strong> have the potential to enrich deep model's visual understanding.
          </p>
        </div>
      </div>

      <!--             <div class="media">
                       <h4>Pixel-Level Recognition</h4>
                       <div class="pull-left">
                         <img class="media-object" src="./assets/cv/SiamDW-Pytorch.png" width="150px" height="150px">
                       </div>
                       <div class="media-body">
                         <p class="abstract-text">
                           It's well known to us that object detection is a combinational task of object classification and object localization. While diverse pipelines and enhancement of object detecion has been extensively explored in recent years, relative tasks including few-shot object detection, general object search await further investigation. Object tracking, from a unified perspective, can be deemed as a task of object search in video level.
                         </p>
                       </div>
      </div> -->

      <div class="media">
        <h4>Object-Level Recognition</h4>
        <!-- <div class="pull-left">
             <img class="media-object" src="./assets/cv/SiamDW-Pytorch.png" width="150px" height="150px">
        </div> -->
        <div class="media-body">
          <p class="abstract-text">
            Object-level recognition requires both classification and localization of objects of interest in a image. Tranditional object-level recognition includes <strong>object detection</strong> and <strong>instance segmentation</strong> in either image (2D/3D) or video level.
            Related topics under low data regime, such as <strong>few-shot object detection, object tracking</strong> or under weakly-supervised training scheme, such as <strong>weakly-supervised object detection</strong> await further investigation. 
          </p>
        </div>
      </div>

<<<<<<< HEAD
            <!-- Footer
            ================================================== -->
            <!-- <div id="footer" class="visible-phone">
                <h3></h3>
                <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=080808&w=250&t=tt&d=SuyMPMY6OOgAhEdXDS6Xlc-HXmnw9Yb5sLKpDuhtW4s&co=ffffff&cmo=3acc3a&cmn=ff5353&ct=808080"></script>
            </div> -->
=======
      <div class="media">
        <h4>Vision and Langauge</h4>
        <!-- <div class="pull-left">
             <img class="media-object" src="./assets/cv/12in1.png" width="150px" height="150px">
        </div> -->
        <div class="media-body">
          <p class="abstract-text">
            A compelling reson to study language and vision jointly is the promise of language as a universal and natural interface for visual reasoning porblems - useful both in speecifying a wide range of problems and in commuicating AI responses. 
            Related topics include <strong> caption, visual question answering, and referring expression</strong>, etc.
          </p>
>>>>>>> f1edf6dba76222d22c23a290ef21e21b6e265016
        </div>
      </div>
      <br>

      <!-- Footer
           ================================================== -->
      <!--<footer class="footer visible-phone">
        <h3></h3>
        <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=080808&w=250&t=tt&d=SuyMPMY6OOgAhEdXDS6Xlc-HXmnw9Yb5sLKpDuhtW4s&co=ffffff&cmo=3acc3a&cmn=ff5353&ct=808080"></script>
      </footer> -->
    </div>
  </div>
</body>
</html>
